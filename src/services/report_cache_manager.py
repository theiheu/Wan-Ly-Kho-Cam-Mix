#!/usr/bin/env python3
"""
Report Cache Manager - Qu·∫£n l√Ω cache b√°o c√°o
L∆∞u tr·ªØ v√† qu·∫£n l√Ω cache cho c√°c b√°o c√°o ti√™u th·ª• h√†ng ng√†y
"""

import os
import json
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from collections import defaultdict

class ReportCacheManager:
    """Qu·∫£n l√Ω cache b√°o c√°o ti√™u th·ª• h√†ng ng√†y"""

    def __init__(self):
        """Kh·ªüi t·∫°o manager cache b√°o c√°o"""
        # Use persistent path manager for consistent paths
        from src.utils.persistent_paths import persistent_path_manager

        self.data_dir = persistent_path_manager.data_path
        self.cache_dir = self.data_dir / "cache" / "reports"
        self.reports_dir = persistent_path_manager.reports_path

        print(f"üîß ReportCacheManager initialized:")
        print(f"   üìÅ Data dir: {self.data_dir}")
        print(f"   üìÅ Cache dir: {self.cache_dir}")
        print(f"   üìÅ Reports dir: {self.reports_dir}")

        # ƒê·∫£m b·∫£o th∆∞ m·ª•c cache t·ªìn t·∫°i
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Th·ªùi gian cache h·ª£p l·ªá (gi·ªù)
        self.cache_validity_hours = 24

        # Metadata cache
        self.cache_metadata_file = self.cache_dir / "cache_metadata.json"
        self.cache_metadata = self._load_cache_metadata()

    def _load_cache_metadata(self) -> Dict[str, Any]:
        """T·∫£i metadata cache"""
        try:
            if self.cache_metadata_file.exists():
                with open(self.cache_metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"L·ªói t·∫£i metadata cache: {e}")

        return {
            'cache_entries': {},
            'last_cleanup': datetime.now().isoformat(),
            'total_cache_size': 0
        }

    def _save_cache_metadata(self):
        """L∆∞u metadata cache"""
        try:
            with open(self.cache_metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"L·ªói l∆∞u metadata cache: {e}")

    def _generate_cache_key(self, report_date: str, report_type: str = "daily_consumption",
                          additional_params: Dict = None) -> str:
        """T·∫°o key cache duy nh·∫•t"""
        # T·∫°o chu·ªói ƒë·ªÉ hash
        key_data = {
            'date': report_date,
            'type': report_type,
            'params': additional_params or {}
        }

        # T·∫°o hash MD5
        key_string = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_string.encode('utf-8')).hexdigest()

    def _get_source_file_hash(self, report_date: str) -> Optional[str]:
        """L·∫•y hash c·ªßa file b√°o c√°o g·ªëc"""
        try:
            # S·ª≠ d·ª•ng persistent path thay v√¨ relative path
            from src.utils.persistent_paths import persistent_path_manager
            report_file = persistent_path_manager.reports_path / f"report_{report_date}.json"

            print(f"üîç Checking report file: {report_file}")

            if not report_file.exists():
                print(f"‚ö†Ô∏è Report file not found: {report_file}")
                return None

            # T·∫°o hash t·ª´ n·ªôi dung file v√† th·ªùi gian s·ª≠a ƒë·ªïi
            with open(report_file, 'r', encoding='utf-8') as f:
                content = f.read()

            mtime = report_file.stat().st_mtime
            hash_data = f"{content}_{mtime}"

            hash_value = hashlib.md5(hash_data.encode('utf-8')).hexdigest()
            print(f"‚úÖ Generated hash for {report_date}: {hash_value[:8]}...")
            return hash_value

        except Exception as e:
            print(f"‚ùå Error generating hash for {report_date}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _is_cache_valid(self, cache_key: str, source_hash: str) -> bool:
        """Ki·ªÉm tra cache c√≥ h·ª£p l·ªá kh√¥ng"""
        if cache_key not in self.cache_metadata['cache_entries']:
            return False

        cache_entry = self.cache_metadata['cache_entries'][cache_key]

        # Ki·ªÉm tra th·ªùi gian
        cached_time = datetime.fromisoformat(cache_entry['created_at'])
        now = datetime.now()

        if (now - cached_time).total_seconds() > self.cache_validity_hours * 3600:
            return False

        # Ki·ªÉm tra hash file ngu·ªìn
        if cache_entry.get('source_hash') != source_hash:
            return False

        # Ki·ªÉm tra file cache c√≥ t·ªìn t·∫°i kh√¥ng
        cache_file = self.cache_dir / f"{cache_key}.json"
        if not cache_file.exists():
            return False

        return True

    def get_cached_report(self, report_date: str, report_type: str = "daily_consumption",
                         additional_params: Dict = None) -> Optional[Dict[str, Any]]:
        """L·∫•y b√°o c√°o t·ª´ cache"""
        try:
            # T·∫°o cache key
            cache_key = self._generate_cache_key(report_date, report_type, additional_params)

            # L·∫•y hash file ngu·ªìn
            source_hash = self._get_source_file_hash(report_date)
            if not source_hash:
                return None

            # Ki·ªÉm tra cache c√≥ h·ª£p l·ªá kh√¥ng
            if not self._is_cache_valid(cache_key, source_hash):
                return None

            # T·∫£i d·ªØ li·ªáu cache
            cache_file = self.cache_dir / f"{cache_key}.json"
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

            # C·∫≠p nh·∫≠t th·ªùi gian truy c·∫≠p
            self.cache_metadata['cache_entries'][cache_key]['last_accessed'] = datetime.now().isoformat()
            self._save_cache_metadata()

            print(f"üìã [Cache] Loaded cached report for {report_date} ({report_type})")
            return cached_data

        except Exception as e:
            print(f"L·ªói t·∫£i cache b√°o c√°o {report_date}: {e}")
            return None

    def cache_report(self, report_date: str, report_data: Dict[str, Any],
                    report_type: str = "daily_consumption", additional_params: Dict = None) -> bool:
        """L∆∞u b√°o c√°o v√†o cache"""
        try:
            print(f"üíæ Caching report for {report_date} ({report_type})")

            # T·∫°o cache key
            cache_key = self._generate_cache_key(report_date, report_type, additional_params)
            print(f"üîë Cache key: {cache_key}")

            # L·∫•y hash file ngu·ªìn
            source_hash = self._get_source_file_hash(report_date)
            if not source_hash:
                print(f"‚ö†Ô∏è Cannot generate hash for source file {report_date}")
                # T·∫°o hash t·ª´ d·ªØ li·ªáu b√°o c√°o thay v√¨ file ngu·ªìn
                import json
                data_str = json.dumps(report_data, sort_keys=True)
                source_hash = hashlib.md5(data_str.encode('utf-8')).hexdigest()
                print(f"üîÑ Using data hash instead: {source_hash[:8]}...")

            # ƒê·∫£m b·∫£o cache directory t·ªìn t·∫°i
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            print(f"üìÅ Cache directory: {self.cache_dir}")

            # L∆∞u d·ªØ li·ªáu cache
            cache_file = self.cache_dir / f"{cache_key}.json"
            print(f"üíæ Saving cache to: {cache_file}")

            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)

            # T√≠nh k√≠ch th∆∞·ªõc file
            file_size = cache_file.stat().st_size

            # C·∫≠p nh·∫≠t metadata
            self.cache_metadata['cache_entries'][cache_key] = {
                'report_date': report_date,
                'report_type': report_type,
                'source_hash': source_hash,
                'created_at': datetime.now().isoformat(),
                'last_accessed': datetime.now().isoformat(),
                'file_size': file_size,
                'additional_params': additional_params or {}
            }

            # C·∫≠p nh·∫≠t t·ªïng k√≠ch th∆∞·ªõc cache
            self.cache_metadata['total_cache_size'] = sum(
                entry.get('file_size', 0) for entry in self.cache_metadata['cache_entries'].values()
            )

            # L∆∞u metadata
            self._save_cache_metadata()

            print(f"‚úÖ Report cached successfully: {cache_file.name} ({file_size} bytes)")
            return True

        except Exception as e:
            print(f"‚ùå Error caching report {report_date}: {e}")
            import traceback
            traceback.print_exc()
            return False

    def invalidate_cache(self, report_date: str = None, report_type: str = None) -> int:
        """V√¥ hi·ªáu h√≥a cache"""
        try:
            removed_count = 0
            keys_to_remove = []

            for cache_key, cache_entry in self.cache_metadata['cache_entries'].items():
                should_remove = False

                # N·∫øu ch·ªâ ƒë·ªãnh ng√†y c·ª• th·ªÉ
                if report_date and cache_entry['report_date'] == report_date:
                    should_remove = True

                # N·∫øu ch·ªâ ƒë·ªãnh lo·∫°i b√°o c√°o c·ª• th·ªÉ
                if report_type and cache_entry['report_type'] == report_type:
                    should_remove = True

                # N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh g√¨, x√≥a t·∫•t c·∫£
                if not report_date and not report_type:
                    should_remove = True

                if should_remove:
                    # X√≥a file cache
                    cache_file = self.cache_dir / cache_entry['cache_file']
                    if cache_file.exists():
                        cache_file.unlink()

                    keys_to_remove.append(cache_key)
                    removed_count += 1

            # X√≥a entries kh·ªèi metadata
            for key in keys_to_remove:
                del self.cache_metadata['cache_entries'][key]

            # C·∫≠p nh·∫≠t t·ªïng k√≠ch th∆∞·ªõc
            self.cache_metadata['total_cache_size'] = sum(
                entry.get('file_size', 0) for entry in self.cache_metadata['cache_entries'].values()
            )

            self._save_cache_metadata()

            print(f"üóëÔ∏è [Cache] Invalidated {removed_count} cache entries")
            return removed_count

        except Exception as e:
            print(f"L·ªói v√¥ hi·ªáu h√≥a cache: {e}")
            return 0

    def cleanup_expired_cache(self) -> int:
        """D·ªçn d·∫πp cache h·∫øt h·∫°n"""
        try:
            removed_count = 0
            keys_to_remove = []
            now = datetime.now()

            for cache_key, cache_entry in self.cache_metadata['cache_entries'].items():
                cached_time = datetime.fromisoformat(cache_entry['created_at'])

                # Ki·ªÉm tra th·ªùi gian h·∫øt h·∫°n
                if (now - cached_time).total_seconds() > self.cache_validity_hours * 3600:
                    # X√≥a file cache
                    cache_file = self.cache_dir / cache_entry['cache_file']
                    if cache_file.exists():
                        cache_file.unlink()

                    keys_to_remove.append(cache_key)
                    removed_count += 1

            # X√≥a entries kh·ªèi metadata
            for key in keys_to_remove:
                del self.cache_metadata['cache_entries'][key]

            # C·∫≠p nh·∫≠t metadata
            self.cache_metadata['last_cleanup'] = now.isoformat()
            self.cache_metadata['total_cache_size'] = sum(
                entry.get('file_size', 0) for entry in self.cache_metadata['cache_entries'].values()
            )

            self._save_cache_metadata()

            if removed_count > 0:
                print(f"üßπ [Cache] Cleaned up {removed_count} expired cache entries")

            return removed_count

        except Exception as e:
            print(f"L·ªói d·ªçn d·∫πp cache: {e}")
            return 0

    def get_cache_statistics(self) -> Dict[str, Any]:
        """L·∫•y th·ªëng k√™ cache"""
        try:
            total_entries = len(self.cache_metadata['cache_entries'])
            total_size = self.cache_metadata.get('total_cache_size', 0)

            # Th·ªëng k√™ theo lo·∫°i b√°o c√°o
            type_stats = defaultdict(int)
            for entry in self.cache_metadata['cache_entries'].values():
                type_stats[entry['report_type']] += 1

            # Th·ªëng k√™ theo ng√†y
            recent_entries = 0
            now = datetime.now()
            for entry in self.cache_metadata['cache_entries'].values():
                cached_time = datetime.fromisoformat(entry['created_at'])
                if (now - cached_time).total_seconds() < 24 * 3600:  # 24 gi·ªù qua
                    recent_entries += 1

            return {
                'total_entries': total_entries,
                'total_size_bytes': total_size,
                'total_size_mb': round(total_size / (1024 * 1024), 2),
                'recent_entries_24h': recent_entries,
                'cache_validity_hours': self.cache_validity_hours,
                'type_breakdown': dict(type_stats),
                'last_cleanup': self.cache_metadata.get('last_cleanup'),
                'cache_directory': str(self.cache_dir)
            }

        except Exception as e:
            print(f"L·ªói l·∫•y th·ªëng k√™ cache: {e}")
            return {}

# Global instance
report_cache_manager = ReportCacheManager()

# Convenience functions
def get_cached_daily_report(report_date: str, additional_params: Dict = None) -> Optional[Dict[str, Any]]:
    """L·∫•y b√°o c√°o h√†ng ng√†y t·ª´ cache"""
    return report_cache_manager.get_cached_report(report_date, "daily_consumption", additional_params)

def cache_daily_report(report_date: str, report_data: Dict[str, Any], additional_params: Dict = None) -> bool:
    """L∆∞u b√°o c√°o h√†ng ng√†y v√†o cache"""
    return report_cache_manager.cache_report(report_date, report_data, "daily_consumption", additional_params)

def invalidate_daily_report_cache(report_date: str = None) -> int:
    """V√¥ hi·ªáu h√≥a cache b√°o c√°o h√†ng ng√†y"""
    return report_cache_manager.invalidate_cache(report_date, "daily_consumption")

def cleanup_report_cache() -> int:
    """D·ªçn d·∫πp cache b√°o c√°o h·∫øt h·∫°n"""
    return report_cache_manager.cleanup_expired_cache()

def get_cache_stats() -> Dict[str, Any]:
    """L·∫•y th·ªëng k√™ cache"""
    return report_cache_manager.get_cache_statistics()

class CacheInvalidationService:
    """D·ªãch v·ª• t·ª± ƒë·ªông v√¥ hi·ªáu h√≥a cache khi d·ªØ li·ªáu thay ƒë·ªïi"""

    def __init__(self):
        """Kh·ªüi t·∫°o d·ªãch v·ª•"""
        self.cache_manager = report_cache_manager

        # Use persistent path manager for consistent paths
        from src.utils.persistent_paths import persistent_path_manager
        self.reports_dir = persistent_path_manager.reports_path

        print(f"üîß CacheInvalidationService initialized:")
        print(f"   üìÅ Reports dir: {self.reports_dir}")

        self.file_timestamps = {}
        self._load_file_timestamps()

    def _load_file_timestamps(self):
        """T·∫£i timestamps c·ªßa c√°c file b√°o c√°o"""
        try:
            if self.reports_dir.exists():
                for report_file in self.reports_dir.glob("report_*.json"):
                    if report_file.is_file():
                        self.file_timestamps[str(report_file)] = report_file.stat().st_mtime
                        print(f"üìä Loaded timestamp for: {report_file.name}")
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i timestamps: {e}")

    def check_and_invalidate_changed_files(self) -> List[str]:
        """Ki·ªÉm tra v√† v√¥ hi·ªáu h√≥a cache cho c√°c file ƒë√£ thay ƒë·ªïi"""
        invalidated_reports = []

        try:
            if not self.reports_dir.exists():
                print(f"‚ö†Ô∏è Reports directory does not exist: {self.reports_dir}")
                return invalidated_reports

            for report_file in self.reports_dir.glob("report_*.json"):
                if not report_file.is_file():
                    continue

                file_path = str(report_file)
                current_mtime = report_file.stat().st_mtime

                # Ki·ªÉm tra file m·ªõi ho·∫∑c ƒë√£ thay ƒë·ªïi
                if (file_path not in self.file_timestamps or
                    self.file_timestamps[file_path] != current_mtime):

                    # Tr√≠ch xu·∫•t ng√†y t·ª´ t√™n file
                    report_date = report_file.stem.replace('report_', '')

                    # V√¥ hi·ªáu h√≥a cache
                    removed_count = self.cache_manager.invalidate_cache(report_date, "daily_consumption")

                    if removed_count > 0:
                        invalidated_reports.append(report_date)
                        print(f"üîÑ [Cache Invalidation] Invalidated cache for {report_date} (file changed)")

                    # C·∫≠p nh·∫≠t timestamp
                    self.file_timestamps[file_path] = current_mtime

        except Exception as e:
            print(f"‚ùå L·ªói ki·ªÉm tra thay ƒë·ªïi file: {e}")

        return invalidated_reports

    def monitor_file_changes(self) -> Dict[str, Any]:
        """Gi√°m s√°t thay ƒë·ªïi file v√† tr·∫£ v·ªÅ th·ªëng k√™"""
        invalidated = self.check_and_invalidate_changed_files()

        return {
            'invalidated_reports': invalidated,
            'total_invalidated': len(invalidated),
            'monitored_files': len(self.file_timestamps),
            'last_check': datetime.now().isoformat()
        }

# Global cache invalidation service
cache_invalidation_service = CacheInvalidationService()

def monitor_and_invalidate_cache() -> Dict[str, Any]:
    """Gi√°m s√°t v√† v√¥ hi·ªáu h√≥a cache t·ª± ƒë·ªông"""
    return cache_invalidation_service.monitor_file_changes()



